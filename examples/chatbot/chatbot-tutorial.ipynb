{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation at a high level\n",
    "\n",
    "```python\n",
    "query = \"Capital of\"\n",
    "\n",
    "output = \"\"\n",
    "for i in range(MAX_GENERATED_TOKENS):\n",
    "    output = output + LLM(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. output = Capital of\n",
    "2. output = Capital of France\n",
    "2. output = Capital of France is\n",
    "2. output = Capital of France Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "query = A chat between a curious user and an artificial intelligence assistant.\n",
    "        The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "        USER: My name is Aniket\n",
    "        ASSISTANT:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longchat_template.format(input=\"My name is Aniket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longchat_template.format(input=\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference import LLMInference, prepare_weights\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(prepare_weights(\"meta-llama/Llama-2-7b-chat-hf\"))\n",
    "# model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(prepare_weights(\"lmsys/longchat-7b-16k\"))\n",
    "model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.chat(longchat_template.format(input=\"What is the capital of France?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.chat(longchat_template.format(input=\"Write a poem on Lightning AI\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket?\"))\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "User: My name is Aniket\n",
    "Assistant: Hi, Aniket how are you?\n",
    "\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "USER: My name is Aniket!\n",
    "ASSISTANT: How can I help you Aniket?\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\", history=history))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PromptTemplate doc](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"history\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longchat_prompt_template.format(\n",
    "    input = \"What is my name?\",\n",
    "    history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from llm_chain import LitGPTLLM\n",
    "\n",
    "\n",
    "llm = LitGPTLLM(model=model)\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferWindowMemory(ai_prefix=\"Assistant\", human_prefix=\"User\", k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"hi, I am Aniket\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"What is my name?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"What is the timezone of London?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"PyTorch Lightning is an open-source library developed by Lightning AI team.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"who developed PyTorch Lightning? just give me the name of the team or person and nothing else.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://twitter.com/yanndubs/status/1681644889145237504?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
